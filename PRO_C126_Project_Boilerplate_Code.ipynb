{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanishka002/Chocolate-cake-Recipe/blob/main/PRO_C126_Project_Boilerplate_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Archery-Target** is a game in which the players shoot sharp-pointed arrows at a round target having 10 rings.\n",
        "\n",
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/4de9132a-c71d-42ce-9099-3293e8805fd9.jpg\"> "
      ],
      "metadata": {
        "id": "nUWO5QkC_g-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RL Problem to Solve\n",
        "Hit the center of the target with maximum reward"
      ],
      "metadata": {
        "id": "5QtHLAqv3wP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://s3-whjr-curriculum-uploads.whjr.online/40656a8c-14e2-4dd7-9f9e-4c17669b9182.png\" width=300>\n",
        "\n",
        "\n",
        "Number of **State**: ?\n",
        "\n",
        "Number of **Actions**: ?\n",
        "\n"
      ],
      "metadata": {
        "id": "Osb6FQ74YZtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "M2oIipDmeqap"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reward Matrix\n",
        "Reward Matrix represents states as rows and actions as columns with respective awards values assigned for a given state and action pair."
      ],
      "metadata": {
        "id": "Ujmi3BO54LfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create reward matrix\n",
        "rewards = np.array([\n",
        " [0,1,2,5,10]\n",
        "])\n"
      ],
      "metadata": {
        "id": "OUqPgOl0eh2u"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Take Action Randomly"
      ],
      "metadata": {
        "id": "Af-CAmdfkDQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define shoot()\n",
        "def shoot():\n",
        "  return np.random.randint(0,5)"
      ],
      "metadata": {
        "id": "ibSLCyMyigmK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q matrix\n",
        "\n",
        "**Q-learning** is a reinforcement learning algorithm. Given the current state, it helps to find the best action to be taken by the agent.\n",
        "\n",
        "**Q-matrix** represents reward received after a taking particular action in the current state. Initially, all the elements of the Q-matrix are zeroes.\n"
      ],
      "metadata": {
        "id": "JXKyVT28hHoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Q-matrix\n",
        "q_matrix = np.zeros([5])\n",
        "print(q_matrix)\n"
      ],
      "metadata": {
        "id": "aNYwOV7ogtw1",
        "outputId": "d50cf5a2-9187-4cc3-cbe4-270521bc2d30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Take Action"
      ],
      "metadata": {
        "id": "0c95A4SOkGdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define take_action\n",
        "\n",
        "def take_action(reward_matrix):\n",
        "  \n",
        "     #Call the shoot() function to get the action\n",
        "     action=shoot()\n",
        "     #Print the action\n",
        "     print(action)\n",
        "     #Get the corresponding reward using Reward matrix\n",
        "     reward=reward_matrix[action]\n",
        "     #Print reward\n",
        "     print(reward)\n",
        "     return action, reward"
      ],
      "metadata": {
        "id": "LSBm-8CJ0UfK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update Q-Matrix"
      ],
      "metadata": {
        "id": "cKy1VkgO4ZhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define run_episode() method\n",
        "\n",
        "def run_episode(reward_matrix, shoot_per_game=5):\n",
        "\n",
        "  score = 0\n",
        "\n",
        "  #use for loop to iterate for number of chances\n",
        "  for shoot_number in range(shoot_per_game):\n",
        "        #print shoot number\n",
        "        print(shoot_number)\n",
        "        #call take_action method to get the action\n",
        "        action, reward = take_action(reward_matrix)\n",
        "        #increase the score    \n",
        "        score = score + reward_matrix[action]\n",
        "        # print shoot number ends\n",
        "        print(shoot_number)\n",
        "  #Update Q-matrix \n",
        "  q_matrix[action] = q_matrix[action] + score/shoot_per_game\n",
        "  #return updated Q-matrix         \n",
        "  return q_matrix\n"
      ],
      "metadata": {
        "id": "_U6NFICkhGMF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Call run_episode method to check the final Q-matrix for one episode\n",
        "q_table = run_episode(rewards)\n",
        "\n",
        "print(q_table)\n"
      ],
      "metadata": {
        "id": "n5pcUO-5pJ-h",
        "outputId": "13d520a7-44e9-4416-d49a-a1c885e54c6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-11ddae8cfe5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Call run_episode method to check the final Q-matrix for one episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mq_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-6c50f1b20f0d>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(reward_matrix, shoot_per_game)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshoot_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#call take_action method to get the action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m#increase the score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreward_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-46b758121ae9>\u001b[0m in \u001b[0;36mtake_action\u001b[0;34m(reward_matrix)\u001b[0m\n\u001b[1;32m      8\u001b[0m      \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m      \u001b[0;31m#Get the corresponding reward using Reward matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m      \u001b[0mreward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreward_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m      \u001b[0;31m#Print reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m      \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 1"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train\n",
        "\n",
        "Create a function that plays  number of games, runs each game for a given number of times, and calculates the mean rewards for each."
      ],
      "metadata": {
        "id": "0dVY734TlZBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define train() function\n",
        "\n",
        "def train(episodes):\n",
        "  #Use for loop to iterate through episodes\n",
        "  for episode in range(episodes):\n",
        "    #Initialize total_reward variable\n",
        "    total_reward = 0\n",
        "    #Print episode start with episode number\n",
        "    print(episode)\n",
        "    #Call run_episode() method to get the q-matrix for one episode\n",
        "    episode_q_matrix = run_episode(rewards)\n",
        "    #Episode reward will be sum of all the rewards for one episode\n",
        "    episode_reward = sum(episode_q_matrix)\n",
        "    #print episode reward\n",
        "    print(episode_reward)\n",
        "    #Total reward will be sum of all the episode reward\n",
        "    total_reward = total_reward + episode_reward\n",
        "    print(episode)\n",
        "  #return total_reward\n",
        "  return total_reward"
      ],
      "metadata": {
        "id": "smfOXGYZlp7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train for 1000 episodes"
      ],
      "metadata": {
        "id": "7WYH_ioykcBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Run the train() function for 1000 episodes\n",
        "number_of_episodes = 1000\n",
        "r = train(number_of_episodes)\n",
        "\n",
        "#Print Average reward\n",
        "print(\"Average Total Reward :\", r/number_of_episodes)\n"
      ],
      "metadata": {
        "id": "cbjnp3PSovsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion: \n",
        "\n",
        "This gives us a good idea about the general performance of the simplest reinforcement learning with **one state multiple action** problem also known as \"**K-Armed Bandit**\" problem.\n",
        "\n",
        "One of the major use cases of this type of problem can be seen in selecting the right advertisement out many to be displayed on the web page. The machine can be taught to pick the best advertisement with the most user clicks!!\n",
        "\n"
      ],
      "metadata": {
        "id": "-jBnwogyuJP0"
      }
    }
  ]
}